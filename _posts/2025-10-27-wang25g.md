---
title: Confidence-Aware Contrastive Distillation for Test-time Prompt Tuning
abstract: Pre-trained vision-language models like CLIP have shown strong performance
  on various visual recognition tasks but often suffer from poor generalization under
  distribution shifts. Test-Time Prompt Tuning (TPT) is a promising solution that
  adapts prompt embeddings during inference using entropy minimization on unlabeled
  test data, while keeping the vision and text encoders frozen. However, entropy-based
  tuning lacks structural regularization and can lead to overconfident misclassifications.
  In this paper, we introduce Confidence-Aware Contrastive Distillation (CaCoD), a
  lightweight and effective approach to improve the robustness and calibration of
  TPT. Our method leverages the confidence structure of test-time predictions by identifying
  high- and low-confidence samples, and aligning their feature representations through
  a contrastive distillation loss. This encourages semantically meaningful updates
  to the prompt embeddings without requiring labels or retraining. Experiments across
  11 fine-grained datasets demonstrate that CaCoD consistently reduces calibration
  error and improves predictive reliability, while maintaining strong accuracy. Our
  approach is model-agnostic and easily pluggable into existing TPT pipelines.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang25g
month: 0
tex_title: Confidence-Aware Contrastive Distillation for Test-time Prompt Tuning
firstpage: 660
lastpage: 666
page: 660-666
order: 660
cycles: false
bibtex_author: Wang, Min and Cheng, Qing
author:
- given: Min
  family: Wang
- given: Qing
  family: Cheng
date: 2025-10-27
address:
container-title: Proceedings of 2025 2nd International Conference on Machine Learning
  and Intelligent Computing
volume: '278'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 10
  - 27
pdf: https://raw.githubusercontent.com/mlresearch/v278/main/assets/wang25g/wang25g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
