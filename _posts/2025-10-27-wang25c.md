---
title: A Study on Japanese-English Machine Translation Based on Large Language Models
  and Post-Editing Strategies
abstract: ' This study investigates the role of post-editing in enhancing neural machine
  translation (NMT) quality, focusing on Japanese-to-English translations in the Information
  and Communication Technology (ICT) sector. By analyzing outputs from three NMT platforms
  (DeepSeek, Youdao, DeepL) against an official benchmark, the research identifies
  persistent challenges, including inconsistent terminology (e.g., “critical infrastructure"
  vs. “core facilities"), tense inaccuracies (“had been restored" vs. “have returned"),
  and omissions of technical annotations (e.g., “Hikari (fiber-optic)"). While DeepL
  and DeepSeek demonstrate superior semantic and structural fidelity, their outputs
  require adjustments to align with domain-specific standards. The proposed post-editing
  framework prioritizes terminological alignment with authoritative references, temporal
  precision to emphasize ongoing actions, and structural coherence to restore source-text
  logic. Full post-editing is advocated for formal contexts to achieve human parity,
  whereas light post-editing suffices for rapid delivery with minimal quality compromises.
  Industry data highlights the dominance of the “machine translation + post-editing"
  model, adopted in 30.4% of projects in 2023, underscoring its efficiency and cost-effectiveness.
  However, human expertise remains irreplaceable in addressing nuanced challenges
  such as cultural adaptation and contextual dependencies. The study concludes by
  advocating for AI-augmented post-editing tools to streamline workflows while preserving
  the “humanistic core" essential for high-stakes translations. This synergy between
  technological advancement and human judgment is critical for advancing translation
  quality in the AI era, particularly in high-demand sectors like ICT.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang25c
month: 0
tex_title: A Study on Japanese-English Machine Translation Based on Large Language
  Models and Post-Editing Strategies
firstpage: 222
lastpage: 228
page: 222-228
order: 222
cycles: false
bibtex_author: Wang, Sirui and Li, Xiang
author:
- given: Sirui
  family: Wang
- given: Xiang
  family: Li
date: 2025-10-27
address:
container-title: Proceedings of 2025 2nd International Conference on Machine Learning
  and Intelligent Computing
volume: '278'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 10
  - 27
pdf: https://raw.githubusercontent.com/mlresearch/v278/main/assets/wang25c/wang25c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
